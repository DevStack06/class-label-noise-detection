{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import print_function, division\n",
    "from builtins import range, input\n",
    "import random\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statistics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "weight_list=[]\n",
    "\n",
    "class AdaBoost:\n",
    "  def __init__(self, M):\n",
    "    self.M = M\n",
    "\n",
    "  def fit(self, X, Y):\n",
    "    self.models = []\n",
    "    self.alphas = []\n",
    "\n",
    "    N, _ = X.shape\n",
    "    W = np.ones(N) / N\n",
    "\n",
    "    for m in range(self.M):\n",
    "      tree = DecisionTreeClassifier(max_depth=1)\n",
    "      tree.fit(X, Y, sample_weight=W)\n",
    "      P = tree.predict(X)\n",
    "\n",
    "      err = W.dot(P != Y)\n",
    "      alpha = 0.5*(np.log(1 - err) - np.log(err))\n",
    "\n",
    "      W = W*np.exp(-alpha*Y*P) # vectorized form\n",
    "      W = W / W.sum() # normalize so it sums to 1\n",
    "      weight_list.append(W)\n",
    "\n",
    "      self.models.append(tree)\n",
    "      self.alphas.append(alpha)\n",
    "\n",
    "  def predict(self, X):\n",
    "    # NOT like SKLearn API\n",
    "    # we want accuracy and exponential loss for plotting purposes\n",
    "    N, _ = X.shape\n",
    "    FX = np.zeros(N)\n",
    "    for alpha, tree in zip(self.alphas, self.models):\n",
    "      FX += alpha*tree.predict(X)\n",
    "    return np.sign(FX), FX\n",
    "\n",
    "  def score(self, X, Y):\n",
    "    # NOT like SKLearn API\n",
    "    # we want accuracy and exponential loss for plotting purposes\n",
    "    P, FX = self.predict(X)\n",
    "    L = np.exp(-Y*FX).mean()\n",
    "    return np.mean(P == Y), L\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    \n",
    "    cancer = datasets.load_breast_cancer()\n",
    "    Y=cancer.target\n",
    "    Y[Y == 0] = -1\n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(cancer.data, Y, test_size=0.2,random_state=109) # 70% training and 30% test\n",
    "   # make the targets -1,+1\n",
    "    per=(Ytrain.shape[0])*(20/100) #20% data will be mislabeled\n",
    "\n",
    "    k=random.sample(range(Ytrain.shape[0]), int(per)) #list of index no. of mislabeled data\n",
    "    for i in k:\n",
    "        if Ytrain[i-1]==1:\n",
    "            Ytrain[i-1]=-1;\n",
    "        else:\n",
    "            Ytrain[i-1]=1\n",
    "#mislabeled data created here till now\n",
    "    model = AdaBoost(10)\n",
    "    model.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_list=np.array(weight_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#observing the wieght of instances\n",
    "#print(wieght_list)\n",
    "a=len(weight_list)\n",
    "\n",
    "#sorting data and finding mean\n",
    "nm=[] \n",
    "nm=np.mean(weight_list, axis=0)\n",
    "#print(nm)\n",
    "a=len(nm)\n",
    "Q=[]\n",
    "x=[]\n",
    "#print(a)\n",
    "for i in range(a):\n",
    "    x=[i,nm[i]]\n",
    "    Q.append(x)\n",
    "    x=[]\n",
    "#print(Q)\n",
    "Q.sort(key = lambda Q: Q[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModeResult(mode=array([0.0013498]), count=array([187]))\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "yp=np.array(nm)\n",
    "mode=stats.mode(nm)\n",
    "print(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "count=Counter(nm)\n",
    "count1=Counter(nm)\n",
    "# print(type(count))\n",
    "nm2=nm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0.00134979518418606: 187, 0.001585267851873107: 80, 0.0054176144064013: 50, 0.004612892279936829: 34, 0.001054331804065297: 26, 0.0010075464525100895: 18, 0.002063354016650229: 17, 0.0036031533498153854: 13, 0.0014775373031769537: 11, 0.0014517563697614235: 8, 0.005049447871051339: 5, 0.0009482366688860665: 3, 0.0012854833812305192: 2, 0.007051462270493987: 1})\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(count1)\n",
    "modeg=statistics.mode(nm)\n",
    "jk=float(count1[0])\n",
    "print(jk)\n",
    "ne=[]\n",
    "for j in range(Xtrain.shape[0]):\n",
    "    if nm2[j]==modeg:\n",
    "        ne.append(j)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 7, 8, 13, 15, 20, 21, 22, 24, 25, 26, 32, 37, 40, 45, 46, 47, 50, 51, 55, 58, 59, 62, 63, 66, 68, 69, 71, 76, 78, 80, 81, 84, 87, 88, 89, 90, 104, 108, 111, 114, 115, 116, 117, 118, 119, 124, 125, 127, 128, 129, 133, 136, 137, 138, 140, 142, 143, 148, 149, 150, 151, 153, 154, 155, 157, 161, 162, 165, 169, 170, 171, 174, 175, 178, 180, 190, 192, 194, 195, 197, 199, 200, 203, 204, 206, 209, 210, 211, 216, 221, 223, 225, 226, 229, 230, 231, 232, 237, 242, 244, 245, 246, 252, 255, 256, 261, 262, 264, 267, 269, 273, 278, 281, 285, 289, 290, 293, 296, 297, 303, 306, 307, 314, 315, 318, 319, 320, 323, 325, 327, 329, 334, 338, 339, 344, 346, 348, 350, 354, 357, 359, 361, 364, 368, 369, 372, 375, 376, 380, 383, 384, 387, 391, 393, 396, 398, 400, 401, 402, 403, 404, 412, 414, 417, 418, 419, 423, 424, 425, 427, 428, 429, 431, 432, 433, 434, 435, 436, 439, 440, 442, 443, 444, 448, 449, 453]\n"
     ]
    }
   ],
   "source": [
    "print(ne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_member(a, b): \n",
    "    a_set = set(a) \n",
    "    b_set = set(b) \n",
    "    \n",
    "    if (a_set & b_set): \n",
    "        wq=a_set & b_set \n",
    "    else: \n",
    "        print(\"No common elements\")  \n",
    "    return wq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187\n",
      "91\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "a=set(ne)\n",
    "b=set(k)\n",
    "re=a&b\n",
    "print(len(ne))\n",
    "print(len(k))\n",
    "print(len(re))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = SVC(kernel='linear') \n",
    "\n",
    "clf1 = KNeighborsClassifier() \n",
    "\n",
    "clf2 = DecisionTreeClassifier() \n",
    "\n",
    "clf3 = GaussianNB() \n",
    "\n",
    "\n",
    "# cancer=datasets.load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9649122807017544\n",
      "0.8245614035087719\n",
      "0.7105263157894737\n",
      "0.9298245614035088\n"
     ]
    }
   ],
   "source": [
    "clf.fit(Xtrain, Ytrain)\n",
    "y_pred_mis = clf.predict(Xtest)\n",
    "clf1.fit(Xtrain, Ytrain)\n",
    "y_pred_mis1 = clf1.predict(Xtest)\n",
    "clf2.fit(Xtrain, Ytrain)\n",
    "y_pred_mis2 = clf2.predict(Xtest)\n",
    "clf3.fit(Xtrain, Ytrain)\n",
    "y_pred_mis3 = clf3.predict(Xtest)\n",
    "\n",
    "\n",
    "print(accuracy_score(Ytest, y_pred_mis))\n",
    "print(accuracy_score(Ytest, y_pred_mis1))\n",
    "print(accuracy_score(Ytest, y_pred_mis2))\n",
    "print(accuracy_score(Ytest, y_pred_mis3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ne:\n",
    "    if(Ytrain[i]==1):\n",
    "        Ytrain[i]=-1\n",
    "    else:\n",
    "        Ytrain[i]=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(Xtrain, Ytrain)\n",
    "y_pred_mis = clf.predict(Xtest)\n",
    "clf1.fit(Xtrain, Ytrain)\n",
    "y_pred_mis1 = clf1.predict(Xtest)\n",
    "clf2.fit(Xtrain, Ytrain)\n",
    "y_pred_mis2 = clf2.predict(Xtest)\n",
    "clf3.fit(Xtrain, Ytrain)\n",
    "y_pred_mis3 = clf3.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3508771929824561\n",
      "0.32456140350877194\n",
      "0.34210526315789475\n",
      "0.2543859649122807\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(Ytest, y_pred_mis))\n",
    "print(accuracy_score(Ytest, y_pred_mis1))\n",
    "print(accuracy_score(Ytest, y_pred_mis2))\n",
    "print(accuracy_score(Ytest, y_pred_mis3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
